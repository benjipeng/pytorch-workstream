{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation with pyTorch"
      ],
      "metadata": {
        "id": "AAaD6issMDA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSKQwqVBMRQ0",
        "outputId": "f86e88a6-ea36-4ac5-dd05-1ddaf6bf53f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation and Autograd\n",
        "> `PyTorch` autograd powers automatic differentiation. When performing operations on Tensors that are set with `requires_grad = True`, PyTorch builds a **Dynamic Computation Graph (DCG)**.\n",
        "`torch.Tensor` is a fundamental data structure:\n",
        "- `.grad`: An attribute of a Tensor that stores the gradient computed by `autograd` after `.backward()` is called *(it is None by default)*.\n",
        "- `.grad_fn`: An attribute of a Tensor that refers to the function that created it."
      ],
      "metadata": {
        "id": "FbKF39xPMuQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Scalar Tensors\n",
        "1. Create `x` with `requires_grad = True`.\n",
        "2. `y`, `z` are derived from `x`"
      ],
      "metadata": {
        "id": "5GPAaDGuNjXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(f\"x: {x}, x.requires_grad: {x.requires_grad}, x.grad_fn: {x.grad_fn}\")\n",
        "\n",
        "y = x ** 2\n",
        "print(f\"y: {y}, y.requires_grad: {y.requires_grad}, y.grad_fn: {y.grad_fn}\")\n",
        "\n",
        "z = 3 * y + 5\n",
        "print(f\"z: {z}, z.requires_grad: {z.requires_grad}, z.grad_fn: {z.grad_fn}\")\n",
        "\n",
        "z.backward()\n",
        "print(f\"Gradient of z with respect to x (dz/dx): {x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk-n31xQNyde",
        "outputId": "afe29646-95ae-4bc7-9b62-81da60ecd730"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: 2.0, x.requires_grad: True, x.grad_fn: None\n",
            "y: 4.0, y.requires_grad: True, y.grad_fn: <PowBackward0 object at 0x7cb133cebd30>\n",
            "z: 17.0, z.requires_grad: True, z.grad_fn: <AddBackward0 object at 0x7cb133cebd30>\n",
            "Gradient of z with respect to x (dz/dx): 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Inputs"
      ],
      "metadata": {
        "id": "Fbb4l4xoQOZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(3.0, requires_grad=True)\n",
        "b = torch.tensor(4.0, requires_grad=True)\n",
        "\n",
        "c = 2 * a + b ** 2\n",
        "print(f\"c: {c}, c.grad_fn: {c.grad_fn}\")\n",
        "\n",
        "c.backward()\n",
        "print(f\"Gradient of c w.r.t. a: {a.grad}\")\n",
        "print(f\"Gradient of c w.r.t. b: {b.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epYtRoodQau_",
        "outputId": "bc7845e6-41f0-457e-cc59-e5aaaca2f491"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c: 22.0, c.grad_fn: <AddBackward0 object at 0x7cb133c4ad70>\n",
            "Gradient of c w.r.t. a: 2.0\n",
            "Gradient of c w.r.t. b: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if a.grad is not None: a.grad.zero_()\n",
        "if b.grad is not None: b.grad.zero_()\n",
        "print(f\"{a.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBXtlEktRObS",
        "outputId": "9c260646-f77d-452a-c300-d42feb9291b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Vector/Matrix Tensors"
      ],
      "metadata": {
        "id": "8TU58rfyRm6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "print(f\"x: {x}\")\n",
        "\n",
        "y = x ** 2\n",
        "print(f\"y: {y}, y.grad_fn: {y.grad_fn}\")\n",
        "\n",
        "z = y.sum()\n",
        "print(f\"z: {z}, z.grad_fn: {z.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMsof8HjSDUc",
        "outputId": "37e8e1c4-4a27-4d6f-8d1c-7aad18b4591e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([1., 2., 3.], requires_grad=True)\n",
            "y: tensor([1., 4., 9.], grad_fn=<PowBackward0>), y.grad_fn: <PowBackward0 object at 0x7cb133a92710>\n",
            "z: 14.0, z.grad_fn: <SumBackward0 object at 0x7cb133a92710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(f\"x: {x}\")\n",
        "y = x * 2\n",
        "y_sum = y.sum()\n",
        "print(f\"y : {y}\")\n",
        "\n",
        "prefill_grad = torch.tensor([0.1,1.0,0.01])\n",
        "\n",
        "y.backward(gradient=prefill_grad)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hifxdG9lSZOt",
        "outputId": "7d45d868-4f0a-4c52-ede6-bce824d3067e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([0.9580, 1.3221, 0.8172], requires_grad=True)\n",
            "y : tensor([1.9159, 2.6443, 1.6344], grad_fn=<MulBackward0>)\n",
            "tensor([0.2000, 2.0000, 0.0200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation with NN"
      ],
      "metadata": {
        "id": "yR7At1RzUGWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_numpy = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=np.float32) # 3 samples, 2 features\n",
        "Y_numpy = np.array([[3.0], [7.0], [11.0]], dtype=np.float32)            # 3 samples, 1 output target\n",
        "                                                                    # Target: y = x1 + x2\n",
        "X = torch.from_numpy(X_numpy)\n",
        "Y_true = torch.from_numpy(Y_numpy)\n",
        "\n",
        "# Parameters (weights and bias) for a linear layer: output = X @ W + b\n",
        "# Input features: 2, Output features: 1\n",
        "W = torch.randn(2, 1, requires_grad=True) # Shape: (num_input_features, num_output_features)\n",
        "b = torch.randn(1, 1, requires_grad=True) # Shape: (1, num_output_features) - will broadcast\n",
        "\n",
        "print(f\"Initial W:\\\\n{W}\")\n",
        "print(f\"Initial b:\\\\n{b}\")\n",
        "\n",
        "# --- Forward Pass ---\n",
        "# Y_pred = X.mm(W) + b  (mm is matrix multiplication)\n",
        "# Or using @ operator for matrix multiplication\n",
        "Y_pred = X @ W + b\n",
        "print(f\"Predictions Y_pred:\\\\n{Y_pred}\")\n",
        "\n",
        "# --- Loss Calculation (Mean Squared Error) ---\n",
        "# loss = (1/N) * sum((Y_pred - Y_true)^2)\n",
        "loss = torch.mean((Y_pred - Y_true) ** 2)\n",
        "print(f\"Initial Loss: {loss.item()}\") # .item() gets scalar value from a single-element tensor\n",
        "# --- Backward Pass ---\n",
        "# This computes d(loss)/dW and d(loss)/db\n",
        "loss.backward()\n",
        "# Gradients are now available in W.grad and b.grad\n",
        "print(f\"Gradient d(loss)/dW:\\\\n{W.grad}\")\n",
        "print(f\"Gradient d(loss)/db:\\\\n{b.grad}\")\n",
        "# --- Optimizer Step (Conceptual) ---\n",
        "# In a real training loop, an optimizer would use these gradients.\n",
        "# For example, with SGD: W = W - learning_rate * W.grad\n",
        "learning_rate = 0.01\n",
        "# W = W - learning_rate * W.grad # This creates a new W, breaking computation graph for requires_grad\n",
        "# For in-place updates that are tracked by autograd for parameters, do:\n",
        "with torch.no_grad(): # IMPORTANT: Optimizer updates should not be part of gradient computation\n",
        "    W -= learning_rate * W.grad\n",
        "    b -= learning_rate * b.grad\n",
        "# --- Zero Gradients ---\n",
        "# After updating weights, gradients must be zeroed for the next iteration\n",
        "W.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(f\"W after one update:\\\\n{W}\")\n",
        "print(f\"b after one update:\\\\n{b}\")\n",
        "# Let's do one more forward pass to see if loss decreased\n",
        "Y_pred_new = X @ W + b\n",
        "loss_new = torch.mean((Y_pred_new - Y_true) ** 2)\n",
        "print(f\"Loss after one update: {loss_new.item()}\") # Should be less than initial loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ZxPKZ8VhCd",
        "outputId": "1742664f-6249-43b5-ba2c-246bd7896943"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial W:\\ntensor([[-1.9245],\n",
            "        [ 0.4336]], requires_grad=True)\n",
            "Initial b:\\ntensor([[0.6641]], requires_grad=True)\n",
            "Predictions Y_pred:\\ntensor([[-0.3933],\n",
            "        [-3.3753],\n",
            "        [-6.3572]], grad_fn=<AddBackward0>)\n",
            "Initial Loss: 140.14463806152344\n",
            "Gradient d(loss)/dW:\\ntensor([[ -80.8702],\n",
            "        [-101.6207]])\n",
            "Gradient d(loss)/db:\\ntensor([[-20.7505]])\n",
            "W after one update:\\ntensor([[-1.1158],\n",
            "        [ 1.4498]], requires_grad=True)\n",
            "b after one update:\\ntensor([[0.8716]], requires_grad=True)\n",
            "Loss after one update: 20.921161651611328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize data and parameters for this snippet\n",
        "X = torch.from_numpy(X_numpy)\n",
        "Y_true = torch.from_numpy(Y_numpy)\n",
        "# Define a simple model using nn.Module\n",
        "class SimpleLinearModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SimpleLinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim) # W and b are created here\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "model = SimpleLinearModel(input_dim, output_dim)\n",
        "print(\"Model structure:\\\\n\", model)\n",
        "print(\"\\\\nModel parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "\n",
        "# Define a loss function\n",
        "criterion = nn.MSELoss() # Mean Squared Error loss\n",
        "\n",
        "# Define an optimizer\n",
        "# It will manage the parameters of 'model'\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# --- Training Loop (Simplified single step) ---\n",
        "\n",
        "# 1. Forward pass\n",
        "Y_pred = model(X)\n",
        "\n",
        "# 2. Compute loss\n",
        "loss = criterion(Y_pred, Y_true)\n",
        "print(f\"\\\\nInitial Loss: {loss.item()}\")\n",
        "\n",
        "# 3. Zero gradients (IMPORTANT: before backward pass)\n",
        "optimizer.zero_grad()\n",
        "# Alternatively, for specific tensors: model.linear.weight.grad.zero_() if it exists\n",
        "\n",
        "# 4. Backward pass (compute gradients)\n",
        "loss.backward()\n",
        "\n",
        "# Gradients are now in model.linear.weight.grad and model.linear.bias.grad\n",
        "print(f\"Gradient d(loss)/d(model.linear.weight):\\\\n{model.linear.weight.grad}\")\n",
        "print(f\"Gradient d(loss)/d(model.linear.bias):\\\\n{model.linear.bias.grad}\")\n",
        "\n",
        "# 5. Optimizer step (update parameters)\n",
        "optimizer.step()\n",
        "\n",
        "# Check updated parameters\n",
        "print(\"\\\\nParameters after one update:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "\n",
        "# Let's do one more forward pass to see if loss decreased\n",
        "Y_pred_new = model(X)\n",
        "loss_new = criterion(Y_pred_new, Y_true)\n",
        "print(f\"Loss after one update: {loss_new.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ZbtP8XWsfa",
        "outputId": "ac200071-c4b1-487d-a9ad-e2183b29fdd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure:\\n SimpleLinearModel(\n",
            "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n",
            "\\nModel parameters:\n",
            "linear.weight tensor([[-0.2435,  0.2167]])\n",
            "linear.bias tensor([-0.1473])\n",
            "\\nInitial Loss: 60.11170959472656\n",
            "Gradient d(loss)/d(model.linear.weight):\\ntensor([[-52.8770, -66.8994]])\n",
            "Gradient d(loss)/d(model.linear.bias):\\ntensor([-14.0224])\n",
            "\\nParameters after one update:\n",
            "linear.weight tensor([[0.2853, 0.8857]])\n",
            "linear.bias tensor([-0.0071])\n",
            "Loss after one update: 8.638227462768555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq81KBV4Wxki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}