{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdd2f2c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This cell just imports PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b64610",
   "metadata": {},
   "source": [
    "`!pip install colab-xterm` to install a terminal if needed. `%reload_ext colabxterm` to reload and use `%xterm` to launch. `%lsmagic` is useful too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Simple Model Definition\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=10, out_features=5) # 10 input features, 5 output features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=5, out_features=1)  # 5 input features, 1 output feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "print(\"SimpleModel class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Instantiation and Initial Parameter Inspection\n",
    "model = SimpleModel()\n",
    "print(\"Model Instantiated:\\n\", model)\n",
    "\n",
    "# Let's inspect the initial weights of the first linear layer\n",
    "print(\"\\nInitial weights of linear1 (first 2 rows):\")\n",
    "print(model.linear1.weight.data[:2])\n",
    "\n",
    "# Store a reference to this specific parameter tensor to track changes\n",
    "initial_linear1_weight_param = model.linear1.weight\n",
    "print(f\"\\nID of linear1.weight tensor: {id(initial_linear1_weight_param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Optimizer Instantiation and Initial Learning Rate\n",
    "initial_lr = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "print(\"Optimizer Instantiated (SGD).\")\n",
    "print(f\"Initial Learning Rate in Optimizer: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scheduler Instantiation and Effect on Optimizer's LR\n",
    "# StepLR decays the learning rate by 'gamma' every 'step_size' epochs.\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "print(\"StepLR Scheduler Instantiated (step_size=2, gamma=0.5).\")\n",
    "\n",
    "print(f\"\\n--- Simulating Epochs/Scheduler Steps ---\")\n",
    "for i in range(5):\n",
    "    # In a real loop, optimizer.step() would happen before scheduler.step() typically for epoch-based schedulers\n",
    "    # But here we just want to see the LR change due to the scheduler.\n",
    "    # We also typically call scheduler.step() at the end of an epoch.\n",
    "    scheduler.step() # Simulate an epoch passing\n",
    "    current_lr_in_optimizer = optimizer.param_groups[0]['lr']\n",
    "    print(f\"After scheduler step {i+1}: Optimizer LR = {current_lr_in_optimizer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: A Single Training Step - Loss, Backward, Optimizer Step\n",
    "\n",
    "# 0. Reset model and optimizer for a clean demonstration for this cell\n",
    "model = SimpleModel() # Re-initialize model to get fresh parameters\n",
    "initial_lr_for_step_demo = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr_for_step_demo)\n",
    "# For this demo, we'll use a fixed LR from the optimizer, not the scheduler from Cell 5\n",
    "print(f\"Model and Optimizer reset. Current Optimizer LR: {optimizer.param_groups[0]['lr']}\\n\")\n",
    "\n",
    "# 1. Create dummy input data and target\n",
    "dummy_input = torch.randn(1, 10) # Batch size 1, 10 input features\n",
    "dummy_target = torch.randn(1, 1) # Batch size 1, 1 output feature\n",
    "\n",
    "# 2. Define a loss function\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "\n",
    "# --- Before any updates ---\n",
    "print(\"--- BEFORE BACKWARD & OPTIMIZER STEP ---\")\n",
    "param_to_watch = model.linear1.weight # The actual nn.Parameter object\n",
    "print(f\"Value of linear1.weight[0,0] BEFORE: {param_to_watch.data[0,0].item():.6f}\")\n",
    "if param_to_watch.grad is not None:\n",
    "    print(f\"Gradient of linear1.weight[0,0] BEFORE: {param_to_watch.grad[0,0].item():.6f}\")\n",
    "else:\n",
    "    print(f\"Gradient of linear1.weight[0,0] BEFORE: None (as expected)\")\n",
    "\n",
    "# 3. Forward pass: Get model's prediction\n",
    "output = model(dummy_input)\n",
    "\n",
    "# 4. Calculate loss\n",
    "loss = criterion(output, dummy_target)\n",
    "print(f\"\\nCalculated Loss: {loss.item():.6f}\")\n",
    "\n",
    "# 5. Zero previous gradients (important!)\n",
    "optimizer.zero_grad()\n",
    "print(\"Optimizer gradients zeroed.\")\n",
    "if param_to_watch.grad is not None: # Should be None or zeros after zero_grad()\n",
    "     print(f\"Gradient of linear1.weight[0,0] after zero_grad: {param_to_watch.grad[0,0].item() if param_to_watch.grad is not None else 'None'}\")\n",
    "else:\n",
    "    print(f\"Gradient of linear1.weight[0,0] after zero_grad: None\")\n",
    "\n",
    "\n",
    "# 6. Backward pass: Compute gradients\n",
    "loss.backward()\n",
    "print(\"\\n--- AFTER loss.backward() ---\")\n",
    "print(f\"Gradient of linear1.weight[0,0] AFTER backward(): {param_to_watch.grad[0,0].item():.6f}\")\n",
    "print(f\"Value of linear1.weight[0,0] (still unchanged): {param_to_watch.data[0,0].item():.6f}\")\n",
    "\n",
    "\n",
    "# 7. Optimizer step: Update parameters\n",
    "optimizer.step()\n",
    "print(\"\\n--- AFTER optimizer.step() ---\")\n",
    "print(f\"Gradient of linear1.weight[0,0] AFTER step (may be unchanged or None): {param_to_watch.grad[0,0].item() if param_to_watch.grad is not None else 'None'}\") # Grads are used up, not zeroed by step\n",
    "print(f\"Value of linear1.weight[0,0] AFTER step (SHOULD BE DIFFERENT): {param_to_watch.data[0,0].item():.6f}\")\n",
    "\n",
    "# Confirm we are looking at the same tensor object that was initially created\n",
    "print(f\"\\nID of linear1.weight tensor now: {id(model.linear1.weight)}\")\n",
    "print(f\"Matches initial ID: {id(model.linear1.weight) == id(initial_linear1_weight_param) if 'initial_linear1_weight_param' in locals() else 'initial_linear1_weight_param not from this cell run'}\")\n",
    "# Note: If you re-ran Cell 3 without re-running this cell, the ID comparison might be tricky.\n",
    "# The key is that model.linear1.weight is the object the optimizer updates."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
