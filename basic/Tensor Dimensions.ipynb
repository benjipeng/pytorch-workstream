{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libs"
      ],
      "metadata": {
        "id": "EI5QG5bIviJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For potentially nicer visualizations\n",
        "\n",
        "# Set a consistent style for plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB42gzl_wEAg",
        "outputId": "44f9b17e-038c-4f39-d55c-ee84ffb23df7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124\n",
            "NumPy Version: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PyTorch ---\n",
        "pt_scalar = torch.tensor(5.0)\n",
        "pt_vector = torch.tensor([1, 2, 3, 4])\n",
        "pt_matrix = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "pt_3d_tensor = torch.randn(2, 3, 4) # Batch=2, Channels/Height=3, Width=4 (common for images)\n",
        "pt_4d_tensor = torch.rand(10, 3, 32, 32) # Batch=10, Channels=3, Height=32, Width=32 (e.g., mini-batch of RGB images)"
      ],
      "metadata": {
        "id": "WQXMsC12wNU_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NumPy ---\n",
        "np_scalar = np.array(5.0)\n",
        "np_vector = np.array([1, 2, 3, 4])\n",
        "np_matrix = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "np_3d_array = np.random.randn(2, 3, 4)\n",
        "np_4d_array = np.random.rand(10, 3, 32, 32)"
      ],
      "metadata": {
        "id": "g253JgA7wUXr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- PyTorch Examples ---\")\n",
        "print(f\"Scalar: {pt_scalar}\")\n",
        "print(f\"Vector: {pt_vector}\")\n",
        "print(f\"Matrix:\\n{pt_matrix}\")\n",
        "print(f\"3D Tensor (sample element): {pt_3d_tensor[0,0,0]}\") # Just to show it's populated\n",
        "print(f\"4D Tensor (sample element): {pt_4d_tensor[0,0,0,0]}\")\n",
        "\n",
        "print(\"\\n--- NumPy Examples ---\")\n",
        "print(f\"Scalar: {np_scalar}\")\n",
        "print(f\"Vector: {np_vector}\")\n",
        "print(f\"Matrix:\\n{np_matrix}\")\n",
        "print(f\"3D Array (sample element): {np_3d_array[0,0,0]}\")\n",
        "print(f\"4D Array (sample element): {np_4d_array[0,0,0,0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk3xfMSlwWuP",
        "outputId": "9e49b62f-6b53-46c5-e35e-56de4b39cb9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch Examples ---\n",
            "Scalar: 5.0\n",
            "Vector: tensor([1, 2, 3, 4])\n",
            "Matrix:\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "3D Tensor (sample element): -0.3703157305717468\n",
            "4D Tensor (sample element): 0.34173113107681274\n",
            "\n",
            "--- NumPy Examples ---\n",
            "Scalar: 5.0\n",
            "Vector: [1 2 3 4]\n",
            "Matrix:\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "3D Array (sample element): -1.2411246493987331\n",
            "4D Array (sample element): 0.8087239870305448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensions"
      ],
      "metadata": {
        "id": "AjI87UV3wakC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- PyTorch Shapes ---\")\n",
        "print(f\"pt_scalar shape: {pt_scalar.shape}\") # torch.Size([])\n",
        "print(f\"pt_vector shape: {pt_vector.shape}\") # torch.Size([4])\n",
        "print(f\"pt_matrix shape: {pt_matrix.shape}\") # torch.Size([3, 2])\n",
        "print(f\"pt_3d_tensor shape: {pt_3d_tensor.shape}\") # torch.Size([2, 3, 4])\n",
        "print(f\"pt_4d_tensor shape: {pt_4d_tensor.shape}\") # torch.Size([10, 3, 32, 32])\n",
        "\n",
        "print(\"\\n--- NumPy Shapes ---\")\n",
        "print(f\"np_scalar shape: {np_scalar.shape}\") # ()\n",
        "print(f\"np_vector shape: {np_vector.shape}\") # (4,)\n",
        "print(f\"np_matrix shape: {np_matrix.shape}\") # (3, 2)\n",
        "print(f\"np_3d_array shape: {np_3d_array.shape}\") # (2, 3, 4)\n",
        "print(f\"np_4d_array shape: {np_4d_array.shape}\") # (10, 3, 32, 32)\n",
        "\n",
        "# Accessing individual dimension sizes\n",
        "print(f\"\\nHeight of pt_matrix (dim 0): {pt_matrix.shape[0]}\")\n",
        "print(f\"Width of pt_matrix (dim 1): {pt_matrix.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaEZYQxvxQhV",
        "outputId": "fc1d22a8-6a7d-4ecf-a692-4fef51243b1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch Shapes ---\n",
            "pt_scalar shape: torch.Size([])\n",
            "pt_vector shape: torch.Size([4])\n",
            "pt_matrix shape: torch.Size([3, 2])\n",
            "pt_3d_tensor shape: torch.Size([2, 3, 4])\n",
            "pt_4d_tensor shape: torch.Size([10, 3, 32, 32])\n",
            "\n",
            "--- NumPy Shapes ---\n",
            "np_scalar shape: ()\n",
            "np_vector shape: (4,)\n",
            "np_matrix shape: (3, 2)\n",
            "np_3d_array shape: (2, 3, 4)\n",
            "np_4d_array shape: (10, 3, 32, 32)\n",
            "\n",
            "Height of pt_matrix (dim 0): 3\n",
            "Width of pt_matrix (dim 1): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Number of Dimensions (`.ndim` / `.dim()`)"
      ],
      "metadata": {
        "id": "dN4DrovYxtMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- PyTorch Number of Dimensions ---\")\n",
        "print(f\"pt_scalar ndim: {pt_scalar.dim()}\") # 0\n",
        "print(f\"pt_vector ndim: {pt_vector.dim()}\") # 1\n",
        "print(f\"pt_matrix ndim: {pt_matrix.dim()}\") # 2\n",
        "print(f\"pt_3d_tensor ndim: {pt_3d_tensor.dim()}\") # 3\n",
        "\n",
        "print(\"\\n--- NumPy Number of Dimensions ---\")\n",
        "print(f\"np_scalar ndim: {np_scalar.ndim}\") # 0\n",
        "print(f\"np_vector ndim: {np_vector.ndim}\") # 1\n",
        "print(f\"np_matrix ndim: {np_matrix.ndim}\") # 2\n",
        "print(f\"np_3d_array ndim: {np_3d_array.ndim}\") # 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatcFxXzxV8g",
        "outputId": "a09057bb-36b4-4522-d6b4-87069b251128"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch Number of Dimensions ---\n",
            "pt_scalar ndim: 0\n",
            "pt_vector ndim: 1\n",
            "pt_matrix ndim: 2\n",
            "pt_3d_tensor ndim: 3\n",
            "\n",
            "--- NumPy Number of Dimensions ---\n",
            "np_scalar ndim: 0\n",
            "np_vector ndim: 1\n",
            "np_matrix ndim: 2\n",
            "np_3d_array ndim: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Number of Elements (`.size` / `.numel()`)"
      ],
      "metadata": {
        "id": "_zWnktPFx5kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- PyTorch Number of Elements ---\")\n",
        "print(f\"pt_scalar numel: {pt_scalar.numel()}\") # 1\n",
        "print(f\"pt_vector numel: {pt_vector.numel()}\") # 4\n",
        "print(f\"pt_matrix numel: {pt_matrix.numel()}\") # 6\n",
        "print(f\"pt_3d_tensor numel: {pt_3d_tensor.numel()}\") # 2*3*4 = 24\n",
        "\n",
        "print(\"\\n--- NumPy Number of Elements ---\")\n",
        "print(f\"np_scalar size: {np_scalar.size}\") # 1\n",
        "print(f\"np_vector size: {np_vector.size}\") # 4\n",
        "print(f\"np_matrix size: {np_matrix.size}\") # 6\n",
        "print(f\"np_3d_array size: {np_3d_array.size}\") # 2*3*4 = 24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCGtA1p7x1j1",
        "outputId": "73aa55c8-b0a6-4510-d433-50d5a5f7ef0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch Number of Elements ---\n",
            "pt_scalar numel: 1\n",
            "pt_vector numel: 4\n",
            "pt_matrix numel: 6\n",
            "pt_3d_tensor numel: 24\n",
            "\n",
            "--- NumPy Number of Elements ---\n",
            "np_scalar size: 1\n",
            "np_vector size: 4\n",
            "np_matrix size: 6\n",
            "np_3d_array size: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Type (`.dtype`)"
      ],
      "metadata": {
        "id": "x_813Hu8yFHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"pt_vector dtype: {pt_vector.dtype}\") # torch.int64\n",
        "print(f\"pt_3d_tensor dtype: {pt_3d_tensor.dtype}\") # torch.float32\n",
        "print(f\"np_vector dtype: {np_vector.dtype}\") # int64\n",
        "print(f\"np_3d_array dtype: {np_3d_array.dtype}\") # float64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLE3bZpIxpsE",
        "outputId": "3ec9be5e-9a1a-460f-dace-0b465ca9bc80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_vector dtype: torch.int64\n",
            "pt_3d_tensor dtype: torch.float32\n",
            "np_vector dtype: int64\n",
            "np_3d_array dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PyTorch ---\n",
        "x_pt = torch.arange(12) # Shape: [12]\n",
        "print(f\"Original x_pt: {x_pt}, shape: {x_pt.shape}\")\n",
        "\n",
        "# Reshape to 3x4\n",
        "x_pt_reshaped = x_pt.reshape(3, 4)\n",
        "print(f\"Reshaped (3,4) x_pt:\\n{x_pt_reshaped}, shape: {x_pt_reshaped.shape}\")\n",
        "\n",
        "# Using -1: PyTorch/NumPy can infer one dimension\n",
        "x_pt_reshaped_infer = x_pt.reshape(2, -1) # Infers 6 for the second dimension\n",
        "print(f\"Reshaped (2,-1) x_pt:\\n{x_pt_reshaped_infer}, shape: {x_pt_reshaped_infer.shape}\")\n",
        "\n",
        "# .view() example\n",
        "# x_pt is contiguous, so view works\n",
        "x_pt_view = x_pt.view(4, 3)\n",
        "print(f\"Viewed (4,3) x_pt:\\n{x_pt_view}, shape: {x_pt_view.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGBxMZosyEmi",
        "outputId": "1eb4e9be-e866-4e17-d126-df9ce60a645a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original x_pt: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), shape: torch.Size([12])\n",
            "Reshaped (3,4) x_pt:\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]]), shape: torch.Size([3, 4])\n",
            "Reshaped (2,-1) x_pt:\n",
            "tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]]), shape: torch.Size([2, 6])\n",
            "Viewed (4,3) x_pt:\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]), shape: torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `.reshape()` or `.view()`\n",
        "- `.view()` requires the tensor's underlying data to be **contiguous** in memory (new shapes but *without* changing the order of data in memory).\n",
        "- `.reshape()`: A more flexible method.\n",
        "    1. Tries to behave as `.view()`\n",
        "    2. Create a copy of the data and re-arrange in memory if the data was not contiguous"
      ],
      "metadata": {
        "id": "5gVg0fb0yUlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `original_tensor` (3x4) is C-contiguous. Its elements are stored in memory like: `0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11`.\n",
        "    - Strides `(4, 1)`: To get to the next row, skip 4 elements; to get to the next column, skip 1 element."
      ],
      "metadata": {
        "id": "kMTn3U0Z4v2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create an original contiguous tensor\n",
        "original_tensor = torch.arange(12).reshape(3, 4)\n",
        "print(f\"Original tensor: \\n{original_tensor}\")\n",
        "print(f\"Original tensor shape: \\n{original_tensor.shape}\")\n",
        "print(f\"Original tensor strides: \\n{original_tensor.stride()}\")\n",
        "print(f\"Is original tensor contiguous? {original_tensor.is_contiguous()}\") # True\n",
        "print(f\"Original tensor data_ptr: {original_tensor.data_ptr()}\") # Memory address"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKIgzNK62Sml",
        "outputId": "bb9abf16-c2cd-490b-a2b2-c33ddced2f78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor: \n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "Original tensor shape: \n",
            "torch.Size([3, 4])\n",
            "Original tensor strides: \n",
            "(4, 1)\n",
            "Is original tensor contiguous? True\n",
            "Original tensor data_ptr: 445753088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Transpose it. This often makes the tensor non-contiguous for certain views.\n",
        "transposed_tensor = original_tensor.transpose(0, 1) # Shape: [4, 3]\n",
        "print(f\"\\nTransposed tensor:\\n{transposed_tensor}\")\n",
        "print(f\"Transposed tensor shape: {transposed_tensor.shape}\")\n",
        "# Strides change: (1, 4). This means to go to the next element in dim 0 (row),\n",
        "# you jump 1 in memory. To go to next element in dim 1 (col), you jump 4 in memory.\n",
        "print(f\"Transposed tensor strides: {transposed_tensor.stride()}\")\n",
        "# It might still be \"contiguous\" in a general sense (e.g. Fortran-contiguous),\n",
        "# but not necessarily C-contiguous in a way that allows all views.\n",
        "print(f\"Is transposed tensor C-contiguous? {transposed_tensor.is_contiguous()}\") # This might be False!\n",
        "print(f\"Transposed tensor data_ptr: {transposed_tensor.data_ptr()}\") # Same memory address as original"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfxLRsJ22Vxi",
        "outputId": "dee3456a-81a1-4850-c4dd-69cac216829e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transposed tensor:\n",
            "tensor([[ 0,  4,  8],\n",
            "        [ 1,  5,  9],\n",
            "        [ 2,  6, 10],\n",
            "        [ 3,  7, 11]])\n",
            "Transposed tensor shape: torch.Size([4, 3])\n",
            "Transposed tensor strides: (1, 4)\n",
            "Is transposed tensor C-contiguous? False\n",
            "Transposed tensor data_ptr: 445753088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Try to .view() the transposed tensor into a flat shape (e.g., 12 elements)\n",
        "try:\n",
        "    # Attempt to view it as a 1D tensor of 12 elements or a 2x6 tensor\n",
        "    viewed_tensor = transposed_tensor.view(12)\n",
        "    # viewed_tensor = transposed_tensor.view(2, 6) # This would also fail for the same reason\n",
        "    print(f\"\\nViewed tensor (this might not be reached if error):\\n{viewed_tensor}\")\n",
        "    print(f\"Viewed tensor data_ptr: {viewed_tensor.data_ptr()}\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"\\nError using .view() on transposed_tensor: {e}\")\n",
        "    print(\"This error occurs because the transposed tensor's memory layout is not compatible with the target view.\")\n",
        "\n",
        "# 4. Now, try .reshape() on the same transposed tensor\n",
        "reshaped_tensor = transposed_tensor.reshape(12)\n",
        "# reshaped_tensor = transposed_tensor.reshape(2, 6) # This would also work\n",
        "print(f\"\\nReshaped tensor:\\n{reshaped_tensor}\")\n",
        "print(f\"Reshaped tensor shape: {reshaped_tensor.shape}\")\n",
        "print(f\"Is reshaped tensor contiguous? {reshaped_tensor.is_contiguous()}\") # True, because reshape made a copy\n",
        "print(f\"Reshaped tensor data_ptr: {reshaped_tensor.data_ptr()}\")\n",
        "# Note: data_ptr for reshaped_tensor will likely be DIFFERENT from transposed_tensor\n",
        "# if a copy was made. If reshape could make a view, it would be the same.\n",
        "\n",
        "print(f\"\\nDid reshape share memory with transposed_tensor? {'Yes' if reshaped_tensor.data_ptr() == transposed_tensor.data_ptr() else 'No, a copy was made.'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvNSLJy3erY",
        "outputId": "7a941a70-0e98-4854-d66d-36305b6139c8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error using .view() on transposed_tensor: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
            "This error occurs because the transposed tensor's memory layout is not compatible with the target view.\n",
            "\n",
            "Reshaped tensor:\n",
            "tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n",
            "Reshaped tensor shape: torch.Size([12])\n",
            "Is reshaped tensor contiguous? True\n",
            "Reshaped tensor data_ptr: 446261952\n",
            "\n",
            "Did reshape share memory with transposed_tensor? No, a copy was made.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to make a view that IS compatible with the transposed tensor's strides\n",
        "# transposed_tensor has shape (4,3) and strides (1,4)\n",
        "# A view to (4,3) should still work as it's the same shape/strides\n",
        "try:\n",
        "    compatible_view = transposed_tensor.view(4,3)\n",
        "    print(f\"\\nCompatible view (4,3) on transposed_tensor works:\\n{compatible_view}\")\n",
        "    print(f\"Is compatible_view contiguous? {compatible_view.is_contiguous()} (same as transposed)\")\n",
        "    print(f\"Compatible_view data_ptr: {compatible_view.data_ptr()} (same as transposed)\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"\\nError with compatible view: {e}\") # Should not happen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYYMqrpG4UhP",
        "outputId": "8c1ac023-9cdb-4255-c6f5-fcedc8199f86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compatible view (4,3) on transposed_tensor works:\n",
            "tensor([[ 0,  4,  8],\n",
            "        [ 1,  5,  9],\n",
            "        [ 2,  6, 10],\n",
            "        [ 3,  7, 11]])\n",
            "Is compatible_view contiguous? False (same as transposed)\n",
            "Compatible_view data_ptr: 445753088 (same as transposed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VNjlVPI4ZhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}