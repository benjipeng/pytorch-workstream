{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KV caching\n",
        "> KV Caching is a cornerstone for efficient inference in modern autoregressive Transformer models."
      ],
      "metadata": {
        "id": "QXA6PPVt1BGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding KV Caching\n",
        "\n",
        "### Autoregressive Generation\n",
        "\n",
        "**What is Autoregressive Generation?**i\n",
        "\n",
        "Autoregressive models, like Large Language Models (LLMs), generate sequences one token (e.g., a word or sub-word) at a time. Each new token prediction depends on all previously generated tokens. Think of writing a story: to choose the next word, you consider the entire story written so far.\n",
        "\n",
        "\n",
        "The self-attention mechanism in Transformers is key to their power. When generating the $t$-th token, the model needs to \"attend\" to all previous tokens $1, \\dots, t-1$. Specifically, the **Query (Q)** vector for the current token interacts with the **Key (K)** and **Value (V)** vectors of all tokens in the current context (itself included).\n",
        "\n",
        "*Q*: Represents the current token asking a question: \"What information is relevant to me?\"\n",
        "\n",
        "*K*: Represents all tokens in the context offering their \"attributes\" or \"topics.\"\n",
        "\n",
        "V: Represents all tokens in the context offering their \"content\" or \"meaning.\""
      ],
      "metadata": {
        "id": "IwhnN5Si1I5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core idea of KV Caching\n",
        "1. Once the Key (K) and Value (V) vectors for a token are computed, cache them.\n",
        "2. When generating the next token:\n",
        "    - Compute Q, K, V only for the *new, current* token.\n",
        "    - Append the new K and V to the *cached* K's and V's from previous tokens.\n",
        "    - Perform the attention calculation using the current Q and the full (cached + new) K's and V's."
      ],
      "metadata": {
        "id": "RK_YzuXEwOxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $x_t$ be the embedding of the $t$-th token. The Query, Key, and Value vectors are typically linear projections:\n",
        "\n",
        "$q_t = x_t W_Q$\n",
        "\n",
        "$k_t = x_t W_K$\n",
        "\n",
        "$v_t = x_t W_V$\n",
        "\n",
        "Where $W_Q, W_K, W_V$ are learnable weight matrices.\n",
        "\n",
        "At generation step $t$:\n",
        "\n",
        "1. Compute $q_t, k_t, v_t$ for the current token $x_t$.\n",
        "2. Retrieve cached keys $K_{cache} = [k_1, \\dots, k_{t-1}]$ and values $V_{cache} = [v_1, \\dots, v_{t-1}]$.\n",
        "3. Form the full key and value sequences for attention:\n",
        "$\\mathbf{K_{total}} = \\text{concat}(K_{cache}, k_t) = [k_1, \\dots, k_{t-1}, k_t]$\n",
        "$\\mathbf{V_{total}} = \\text{concat}(V_{cache}, v_t) = [v_1, \\dots, v_{t-1}, v_t]$\n",
        "4. The attention output for the $t$-th token is then computed as:\n",
        "$\\text{AttentionOutput}*t = \\text{softmax}\\left(\\frac{q_t \\mathbf{K*{total}}^T}{\\sqrt{d_k}}\\right) \\mathbf{V_{total}}$\n",
        "(where $d_k$ is the dimension of key vectors).\n",
        "5. The updated cache for the next step will be $(K_{total}, V_{total})$."
      ],
      "metadata": {
        "id": "oTyIe8cSxw3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Ready\n",
        "\n",
        "Input tensors (`query`, `key`, `value`) have 4-dimensional shape: `[B, N_h, S, D_h]`\n",
        "1. B - Batch Size\n",
        "  - Just the number of independent sequences to process simultaneously.\n",
        "  - Each sequence in the batch is processed in parallel / does NOT interact with others.\n",
        "2. `N_h` - Number of attention heads\n",
        "  - Multi-Head Attention (MHA) allows the model to jointly attend to information from *different* representation subspaces at different positions. Instead of performing one single attention calculation over the full `embed_dim`, the model splits `embed_dim` into `N_h` \"heads.\" Each head has a dimension `D_h` = `embed_dim` / `N_h`.\n",
        "  - Each head performs its own scaled dot-product attention independently. The outputs of these heads are then typically concatenated and linearly projected to get the final result.\n",
        "  - hink of it as having `N_h` different \"experts\" looking at different aspects of the relationships between tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "WnXqUu0XuHxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Helper function for scaled dot-product attention\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Efficient scaled dot-product attention.\n",
        "    query: [B, N_h, S_q, D_h]\n",
        "    key:   [B, N_h, S_k, D_h]\n",
        "    value: [B, N_h, S_v, D_h] (S_k == S_v)\n",
        "    mask: typically for causal attention\n",
        "    \"\"\"\n",
        "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
        "    d_k = query.size(-1)\n",
        "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output # [B, N_h, S_q, D_h]"
      ],
      "metadata": {
        "id": "fgEw9a7Oxx1P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionNoCache(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x_query, x_kv, causal_mask=False):\n",
        "        # x_query: [B, S_q, D_model] (e.g., current token(s) for Q)\n",
        "        # x_kv:    [B, S_kv, D_model] (e.g., full sequence for K, V)\n",
        "        # For self-attention, x_query and x_kv are often the same.\n",
        "        # For cross-attention, they can be different.\n",
        "        # Here, we assume self-attention context where x_query is part of x_kv\n",
        "        # or represents the queries for the full x_kv context.\n",
        "\n",
        "        B, S_q, _ = x_query.shape\n",
        "        _, S_kv, _ = x_kv.shape\n",
        "\n",
        "        q = self.q_proj(x_query) # [B, S_q, D_model]\n",
        "        k = self.k_proj(x_kv)    # [B, S_kv, D_model]\n",
        "        v = self.v_proj(x_kv)    # [B, S_kv, D_model]\n",
        "\n",
        "        # Reshape and transpose for multi-head: [B, N_h, S, D_h]\n",
        "        q = q.view(B, S_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, S_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, S_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        mask = None\n",
        "        if causal_mask:\n",
        "            # Create a causal mask for S_q queries attending to S_kv keys/values\n",
        "            # This is a general mask; for autoregressive generation, S_q might be 1\n",
        "            # and S_kv is the total length.\n",
        "            mask = torch.tril(torch.ones(S_q, S_kv, device=x_query.device)).bool()\n",
        "            # If S_q != S_kv, this mask needs careful handling based on relative positions.\n",
        "            # For S_q = 1 (typical generation), this means the query can attend to all S_kv.\n",
        "            # For S_q = S_kv (prompt processing), this is a standard causal mask.\n",
        "            if S_q == 1 and S_kv > 1: # single query attending to full history\n",
        "                 pass # no upper triangular part to mask for a single query\n",
        "            elif S_q == S_kv: # standard self-attention causal mask\n",
        "                 mask = torch.tril(torch.ones(S_q, S_kv, device=x_query.device)).view(1, 1, S_q, S_kv)\n",
        "            else: # More complex cases, let's assume full attention for simplicity here if not square\n",
        "                  # or handle specific masking outside. For now, just allow all for non-square.\n",
        "                  pass\n",
        "\n",
        "\n",
        "        attn_output = scaled_dot_product_attention(q, k, v, mask=mask) # [B, N_h, S_q, D_h]\n",
        "\n",
        "        # Concatenate heads and project\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, S_q, self.embed_dim)\n",
        "        output = self.o_proj(attn_output) # [B, S_q, D_model]\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "xlkz81_9naD0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage for MultiHeadAttentionNoCache ---\n",
        "\n",
        "# Parameters\n",
        "batch_size_l1 = 2\n",
        "seq_len_l1 = 10      # Sequence length for both query and key/value (self-attention)\n",
        "embed_dim_l1 = 64\n",
        "num_heads_l1 = 4\n",
        "device_l1 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"\\n--- Running Level 1: MultiHeadAttentionNoCache Example ---\")\n",
        "print(f\"Using device: {device_l1}\")\n",
        "\n",
        "# Instantiate the module\n",
        "mha_no_cache_instance = MultiHeadAttentionNoCache(embed_dim_l1, num_heads_l1).to(device_l1)\n",
        "mha_no_cache_instance.eval() # Set to evaluation mode if not training\n",
        "\n",
        "# Create dummy input tensor (e.g., embeddings of a sequence)\n",
        "# For self-attention, x_query and x_kv are typically the same.\n",
        "input_sequence = torch.randn(batch_size_l1, seq_len_l1, embed_dim_l1, device=device_l1)\n",
        "\n",
        "# --- Case 1: Self-attention without causal masking (e.g., Encoder block) ---\n",
        "print(f\"\\nCase 1: Self-attention, no causal mask\")\n",
        "# Here, x_query and x_kv are the same.\n",
        "output_self_attn = mha_no_cache_instance(x_query=input_sequence, x_kv=input_sequence, causal_mask=False)\n",
        "print(f\"Input shape: {input_sequence.shape}\")\n",
        "print(f\"Output shape (self-attention, no mask): {output_self_attn.shape}\")\n",
        "# Expected output shape: [batch_size_l1, seq_len_l1, embed_dim_l1]\n",
        "\n",
        "# --- Case 2: Self-attention with causal masking (e.g., Decoder block during training/prompt processing) ---\n",
        "print(f\"\\nCase 2: Self-attention, with causal mask\")\n",
        "output_causal_self_attn = mha_no_cache_instance(x_query=input_sequence, x_kv=input_sequence, causal_mask=True)\n",
        "print(f\"Input shape: {input_sequence.shape}\")\n",
        "print(f\"Output shape (causal self-attention): {output_causal_self_attn.shape}\")\n",
        "# Expected output shape: [batch_size_l1, seq_len_l1, embed_dim_l1]\n",
        "\n",
        "\n",
        "# --- Case 3: Simulating a single query token attending to a sequence of KV pairs ---\n",
        "# This is closer to what happens token-by-token in naive autoregressive generation,\n",
        "# where the 'query' is the current token and 'kv' is the whole sequence so far.\n",
        "print(f\"\\nCase 3: Single query token, attending to full KV sequence (simulating one step of naive generation)\")\n",
        "current_token_embedding = torch.randn(batch_size_l1, 1, embed_dim_l1, device=device_l1) # S_q = 1\n",
        "full_kv_sequence = torch.randn(batch_size_l1, seq_len_l1, embed_dim_l1, device=device_l1) # S_kv = 10\n",
        "\n",
        "# In this specific setup for MultiHeadAttentionNoCache, if causal_mask=True and S_q=1,\n",
        "# the mask generated internally would allow the single query to attend to all S_kv.\n",
        "# If it were strictly causal based on absolute positions, and the query was \"after\" S_kv, it would be different.\n",
        "# But here, the mask is relative to the input S_q and S_kv lengths.\n",
        "output_single_query = mha_no_cache_instance(x_query=current_token_embedding, x_kv=full_kv_sequence, causal_mask=False) # Causal usually means query can't see \"future\" keys within x_kv. Here, x_query is separate.\n",
        "                                                                                                                       # If causal_mask=True, and S_q=1, the generated mask would be [1,1,1,1,....S_kv] for that one query.\n",
        "print(f\"Query shape: {current_token_embedding.shape}\")\n",
        "print(f\"KV sequence shape: {full_kv_sequence.shape}\")\n",
        "print(f\"Output shape (single query): {output_single_query.shape}\")\n",
        "# Expected output shape: [batch_size_l1, 1, embed_dim_l1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmfV4T69w99x",
        "outputId": "e984d465-7f83-4344-9d12-853430e0d537"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Level 1: MultiHeadAttentionNoCache Example ---\n",
            "Using device: cpu\n",
            "\n",
            "Case 1: Self-attention, no causal mask\n",
            "Input shape: torch.Size([2, 10, 64])\n",
            "Output shape (self-attention, no mask): torch.Size([2, 10, 64])\n",
            "\n",
            "Case 2: Self-attention, with causal mask\n",
            "Input shape: torch.Size([2, 10, 64])\n",
            "Output shape (causal self-attention): torch.Size([2, 10, 64])\n",
            "\n",
            "Case 3: Single query token, attending to full KV sequence (simulating one step of naive generation)\n",
            "Query shape: torch.Size([2, 1, 64])\n",
            "KV sequence shape: torch.Size([2, 10, 64])\n",
            "Output shape (single query): torch.Size([2, 1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "test_tensor = torch.randn([2, 3, 4, 5])\n",
        "test_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48KXRCa2lYzc",
        "outputId": "13fdf66a-d799-4ca6-eacf-cb00f4f77678"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.3487,  1.6347,  0.1443,  0.7240,  0.7381],\n",
              "          [-0.7476, -0.6850,  1.7193,  1.9604,  0.5786],\n",
              "          [-0.2361, -0.5395,  1.2992, -1.2165,  0.8674],\n",
              "          [ 0.9757, -0.8619,  1.1845,  0.6143, -0.6978]],\n",
              "\n",
              "         [[ 0.1655, -1.0265, -1.4400,  1.4185, -1.1353],\n",
              "          [-0.9438,  1.4995, -2.1421, -1.6823,  1.8388],\n",
              "          [-0.1459,  1.8395,  0.2912,  0.8873, -1.1434],\n",
              "          [ 0.2176, -0.6245, -0.1394, -0.4310, -0.8455]],\n",
              "\n",
              "         [[ 0.9501, -1.0886, -0.2018,  1.3524, -0.9115],\n",
              "          [ 0.7123,  0.3665,  0.1948,  1.0899,  1.2612],\n",
              "          [ 0.2031,  0.3736,  0.0554, -0.7537, -0.9879],\n",
              "          [-1.5952, -1.7867, -0.3117,  0.2865, -1.1933]]],\n",
              "\n",
              "\n",
              "        [[[-1.5285,  0.9560,  0.9153, -0.3917,  0.3930],\n",
              "          [-0.6037, -0.3035, -0.4196, -1.4552, -1.7482],\n",
              "          [ 0.8614, -0.9646, -1.6007, -0.0091, -1.1557],\n",
              "          [ 0.7585, -1.7625, -1.2060,  0.8237, -0.8793]],\n",
              "\n",
              "         [[-1.5392,  3.9224,  0.7778,  1.1515,  0.7246],\n",
              "          [-1.6642, -1.2135,  1.0073, -0.5727,  1.5417],\n",
              "          [ 0.5820, -1.1727,  0.9644,  1.3042,  0.2450],\n",
              "          [-0.1246,  0.0563,  1.2829,  0.0783, -1.3167]],\n",
              "\n",
              "         [[ 0.2109, -0.2294, -0.2135, -0.3264, -0.8997],\n",
              "          [-0.8821, -0.9235,  0.7612, -0.2533,  1.0539],\n",
              "          [ 0.2647,  0.1712,  1.7418,  1.8057,  0.6904],\n",
              "          [-0.3666,  2.2779,  1.1772,  0.1004, -0.1102]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(test_tensor, test_tensor.transpose(-2,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXD6LFEfm-Ic",
        "outputId": "a84dfaef-3914-4feb-8ded-da8634100411"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.5808,  1.9831, -0.6166, -2.6242],\n",
              "          [ 1.9831,  8.1619,  0.8968,  2.6980],\n",
              "          [-0.6166,  0.8968,  4.2670,  0.4209],\n",
              "          [-2.6242,  2.6980,  0.4209,  3.9623]],\n",
              "\n",
              "         [[ 6.4555, -3.0848,  0.2252,  1.2263],\n",
              "          [-3.0848, 13.9391, -1.3230, -1.6729],\n",
              "          [ 0.2252, -1.3230,  5.5844, -0.6367],\n",
              "          [ 1.2263, -1.6729, -0.6367,  1.3574]],\n",
              "\n",
              "         [[ 4.7883,  0.5628, -0.3438,  1.9674],\n",
              "          [ 0.5628,  3.4580, -1.7749, -3.0446],\n",
              "          [-0.3438, -1.7749,  1.7279, -0.0459],\n",
              "          [ 1.9674, -3.0446, -0.0459,  7.3402]]],\n",
              "\n",
              "\n",
              "        [[[ 4.3959,  0.1314, -4.1546, -4.6165],\n",
              "          [ 0.1314,  5.8064,  2.4779,  0.9217],\n",
              "          [-4.1546,  2.4779,  5.5701,  5.2926],\n",
              "          [-4.6165,  0.9217,  5.2926,  6.5881]],\n",
              "\n",
              "         [[20.2104, -0.9571, -3.0663,  0.5464],\n",
              "          [-0.9571,  7.9613,  1.0569, -0.6435],\n",
              "          [-3.0663,  1.0569,  4.4052,  0.8783],\n",
              "          [ 0.5464, -0.6435,  0.8783,  3.4043]],\n",
              "\n",
              "         [[ 1.0587, -1.0023, -1.5660, -0.7848],\n",
              "          [-1.0023,  3.3854,  1.2046, -1.0257],\n",
              "          [-1.5660,  1.2046,  6.8706,  2.4486],\n",
              "          [-0.7848, -1.0257,  2.4486,  6.7314]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEDokNF0nG6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}